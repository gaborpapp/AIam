mean normalization: should all training data be mean normalized, even for kernel PCA?
  see --training-data-stats
  testing whether mean normalization helps: measure actual ability to retain original data, and then compare
  perhaps verify this measure with linear PCA as well?

improvisation
  eliminate exception caused by path being too short (occurs now and then)
  parametrize 0.3 in navigator.py
  pull towards periphery also when novelty is low

singular matrix exception:
  rm scenes/valencia_all.bvh.quaternion.10fps.data && python dim_reduce.py hierarchical -r quaternion -bvh scenes/valencia_all.bvh -training-data-frame-rate 10 -train -n 7 --translate --translation-weight 10

automatic detection of whether training data cache can be used (currently, it needs to be removed manually when some parameter such as --translation-weight is changed)
  perhaps using profiles?
  relevant factors: bvh, stimulus, potential entity parameters
    for dim reduction also n_components

MemoryError when running: python dim_reduce.py angle_3dim_quaternion -stimulus joint -bvh scenes/valencia_all.bvh -joint RShoulder -train -n 3
  perhaps not if reverting to linear PCA?

ensure that quaternion discontinuity problem (see README) cannot appear in real cases
  work started with analyze_quaternions.py

real-time interaction
  switch between interaction modes (follow, improvise etc)?
  more real-time modes in addition to "follow"?
  use mapping as output only with old state-based input?

interactive evolution of trajectories?
